{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Reranking with ColBERT: Improving Retrieval Quality\n",
    "**Inspired by:** [RAGatouille Reranking Example](https://github.com/AnswerDotAI/RAGatouille/blob/main/examples/04-reranking.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ragatouille numpy sentence-transformers pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Load ColBERT Model for Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "# Load pre-trained ColBERT v2.0 model\n",
    "# This model will be used for reranking (not indexing)\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "print(\"ColBERT model loaded successfully for reranking!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulating an Existing RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from voyager import Index, Space\n",
    "\n",
    "\n",
    "class ExistingRetrievalPipeline:\n",
    "    \"\"\"Simulates a typical first-stage RAG retrieval pipeline using bi-encoder embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, embedder_name: str = \"BAAI/bge-small-en-v1.5\"):\n",
    "        # Initialize the sentence transformer model\n",
    "        self.embedder = SentenceTransformer(embedder_name)\n",
    "\n",
    "        # Map to store original document content by index ID\n",
    "        self.collection_map = {}\n",
    "\n",
    "        # Create a vector index using cosine similarity\n",
    "        self.index = Index(\n",
    "            Space.Cosine,\n",
    "            num_dimensions=self.embedder.get_sentence_embedding_dimension(),\n",
    "        )\n",
    "\n",
    "        print(f\"Initialized retrieval pipeline with {embedder_name}\")\n",
    "        print(\n",
    "            f\"Embedding dimension: {self.embedder.get_sentence_embedding_dimension()}\"\n",
    "        )\n",
    "\n",
    "    def index_documents(self, documents: list[dict]) -> None:\n",
    "        \"\"\"Index a list of documents into the vector store.\"\"\"\n",
    "        for document in documents:\n",
    "            # Encode document content to vector embedding\n",
    "            embedding = self.embedder.encode(document[\"content\"])\n",
    "\n",
    "            # Add to index and store mapping\n",
    "            idx = self.index.add_item(embedding)\n",
    "            self.collection_map[idx] = document[\"content\"]\n",
    "\n",
    "        print(f\"Indexed {len(documents)} document chunks\")\n",
    "\n",
    "    def query(self, query: str, k: int = 10) -> list[str]:\n",
    "        \"\"\"Retrieve top-k most similar documents for a given query.\"\"\"\n",
    "        # Encode query to vector\n",
    "        query_embedding = self.embedder.encode(query)\n",
    "\n",
    "        # Search index for k nearest neighbors\n",
    "        neighbor_ids = self.index.query(query_embedding, k=k)[0]\n",
    "\n",
    "        # Retrieve original document content\n",
    "        results = [self.collection_map[idx] for idx in neighbor_ids]\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "retrieval_pipeline = ExistingRetrievalPipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare and Index Documents\n",
    "\n",
    "We'll load a text file about climate change and agriculture, then chunk it into smaller pieces suitable for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille.data import CorpusProcessor\n",
    "\n",
    "# Initialize the corpus processor\n",
    "corpus_processor = CorpusProcessor()\n",
    "\n",
    "# Load the document\n",
    "with open(\"text_for_reranking.txt\", \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(f\"Loaded document with {len(content)} characters\")\n",
    "print(\"\\n--- Document Preview ---\")\n",
    "print(content[:300] + \"...\\n\")\n",
    "\n",
    "# Process and chunk the document\n",
    "# chunk_size=100 tokens creates manageable pieces for retrieval\n",
    "documents = corpus_processor.process_corpus([content], chunk_size=100)\n",
    "\n",
    "print(f\"Created {len(documents)} chunks from the document\")\n",
    "print(\"\\n--- Example Chunk ---\")\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the documents in our retrieval pipeline\n",
    "retrieval_pipeline.index_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. First-Stage Retrieval (Without Reranking)\n",
    "\n",
    "Let's query our pipeline with a complex question about climate change and agriculture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our test query\n",
    "query = \"How does climate change affect agriculture, and what are the impacts on crop yields, farming practices, and food security?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Retrieve top 7 results using bi-encoder\n",
    "raw_results = retrieval_pipeline.query(query, k=7)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FIRST-STAGE RETRIEVAL RESULTS (Bi-Encoder)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, result in enumerate(raw_results, 1):\n",
    "    print(f\"\\n[Rank {i}]\")\n",
    "    print(result)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of First-Stage Results\n",
    "\n",
    "Notice that while the results are generally relevant, they may not be optimally ordered. Some highly relevant passages might appear lower in the ranking due to the limitations of bi-encoder similarity matching.\n",
    "\n",
    "**Key observation:** Look for passages that directly answer the question about crop yields and food security. Are they in the top positions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Second-Stage Reranking with ColBERT\n",
    "\n",
    "Now let's use ColBERT to rerank these results. ColBERT uses a more sophisticated scoring mechanism that considers fine-grained token-level interactions between the query and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank the results using ColBERT\n",
    "# We'll return top 5 after reranking\n",
    "reranked_results = RAG.rerank(query=query, documents=raw_results, k=7)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RERANKED RESULTS (ColBERT)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for result in reranked_results:\n",
    "    print(f\"\\n[Rank {result['rank']+1}] (Score: {result['score']:.4f})\")\n",
    "    print(result[\"content\"])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Side-by-Side Comparison\n",
    "\n",
    "Let's compare the top 5 results before and after reranking to see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare comparison data\n",
    "comparison_data = []\n",
    "\n",
    "for i in range(7):\n",
    "    comparison_data.append(\n",
    "        {\n",
    "            \"Rank\": i + 1,\n",
    "            \"Before Reranking (Bi-Encoder)\": raw_results[i][:150] + \"...\",\n",
    "            \"After Reranking (ColBERT)\": reranked_results[i][\"content\"][:150] + \"...\",\n",
    "            \"ColBERT Score\": f\"{reranked_results[i]['score']:.4f}\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"COMPARISON: Before vs After Reranking\")\n",
    "print(\"=\" * 120)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Impact of Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a visualization showing rank changes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left plot: ColBERT scores after reranking\n",
    "ranks = [r[\"rank\"] + 1 for r in reranked_results]\n",
    "scores = [r[\"score\"] for r in reranked_results]\n",
    "\n",
    "ax1.bar(ranks, scores, color=\"steelblue\", alpha=0.7)\n",
    "ax1.set_xlabel(\"Rank After Reranking\", fontsize=12)\n",
    "ax1.set_ylabel(\"ColBERT Relevance Score\", fontsize=12)\n",
    "ax1.set_title(\n",
    "    \"Document Relevance Scores After Reranking\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "ax1.set_xticks(ranks)\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Right plot: Rank changes visualization\n",
    "# Map reranked documents back to their original positions\n",
    "original_positions = []\n",
    "for reranked in reranked_results:\n",
    "    for i, original in enumerate(raw_results):\n",
    "        if reranked[\"content\"] == original:\n",
    "            original_positions.append(i + 1)\n",
    "            break\n",
    "\n",
    "# Plot rank movement\n",
    "for i, (old_pos, new_pos) in enumerate(zip(original_positions, ranks)):\n",
    "    color = \"green\" if old_pos > new_pos else \"red\" if old_pos < new_pos else \"gray\"\n",
    "    ax2.plot(\n",
    "        [0, 1],\n",
    "        [old_pos, new_pos],\n",
    "        \"o-\",\n",
    "        color=color,\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax2.text(-0.1, old_pos, f\"{old_pos}\", fontsize=10, ha=\"right\", va=\"center\")\n",
    "    ax2.text(1.1, new_pos, f\"{new_pos}\", fontsize=10, ha=\"left\", va=\"center\")\n",
    "\n",
    "ax2.set_xlim(-0.3, 1.3)\n",
    "ax2.set_ylim(0, 8)\n",
    "ax2.set_xticks([0, 1])\n",
    "ax2.set_xticklabels([\"Before\\nReranking\", \"After\\nReranking\"], fontsize=11)\n",
    "ax2.set_ylabel(\"Rank Position\", fontsize=12)\n",
    "ax2.set_title(\"Rank Changes from Reranking\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=\"green\", linewidth=2, label=\"Moved Up\"),\n",
    "    Line2D([0], [0], color=\"red\", linewidth=2, label=\"Moved Down\"),\n",
    "    Line2D([0], [0], color=\"gray\", linewidth=2, label=\"No Change\"),\n",
    "]\n",
    "ax2.legend(handles=legend_elements, loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Understanding the Trade-offs\n",
    "\n",
    "### When to Use Reranking\n",
    "\n",
    "**Reranking is ideal when:**\n",
    "- You have an existing retrieval pipeline you don't want to rebuild\n",
    "- You need to improve relevance without major infrastructure changes\n",
    "- Your first-stage retriever is fast but not highly accurate\n",
    "- You can afford slight additional latency for better quality\n",
    "- Your candidate set (k) is moderate (10-100 documents)\n",
    "\n",
    "**Full ColBERT indexing is better when:**\n",
    "- You're building a new system from scratch\n",
    "- You need maximum retrieval quality\n",
    "- You have resources for offline index building\n",
    "- Your query volume justifies the infrastructure\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "Let's measure the computational cost of reranking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices and Tips\n",
    "\n",
    "### Choosing K Values\n",
    "\n",
    "1. **First-stage K (k1)**: Retrieve more candidates than needed\n",
    "   - Typical range: 20-100\n",
    "   - Higher k1 gives reranker more options but increases cost\n",
    "   - Rule of thumb: k1 = 2-5x your final target\n",
    "\n",
    "2. **Reranking K (k2)**: Final number of documents\n",
    "   - Typical range: 3-10\n",
    "   - Depends on your LLM's context window\n",
    "   - More isn't always better (noise vs. signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
