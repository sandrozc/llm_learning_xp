{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Encoder vs Cross-Encoder for Semantic Search\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the difference between bi-encoder and cross-encoder architectures\n",
    "- Implement cosine similarity for vector-based search\n",
    "- Compare performance and use cases for both approaches\n",
    "- Learn when to use each approach in RAG pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence_transformers numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Architectures\n",
    "\n",
    "### Bi-Encoder\n",
    "- Encodes query and documents **independently**\n",
    "- Produces fixed-size embeddings (vectors)\n",
    "- Fast: Can pre-compute document embeddings\n",
    "- Scalable: Use vector databases for efficient search\n",
    "\n",
    "### Cross-Encoder\n",
    "- Encodes query and document **together** as a pair\n",
    "- Produces a relevance score directly\n",
    "- Slow: Must process every (query, document) pair\n",
    "- More accurate: Captures interaction between query and document\n",
    "\n",
    "**Best Practice:** Use bi-encoder for first-stage retrieval, cross-encoder for reranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement Cosine Similarity\n",
    "\n",
    "Cosine similarity measures the angle between two vectors. It's the most common metric for comparing semantic embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "\n",
    "    Formula: cos(θ) = (a · b) / (||a|| × ||b||)\n",
    "\n",
    "    Args:\n",
    "        a: First vector\n",
    "        b: Second vector\n",
    "\n",
    "    Returns:\n",
    "        Similarity score between -1 and 1 (higher is more similar)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test the function\n",
    "vec1 = np.array([1, 2, 3])\n",
    "vec2 = np.array([2, 4, 6])  # Same direction as vec1\n",
    "vec3 = np.array([1, 0, -1])  # Different direction\n",
    "\n",
    "print(f\"Similarity (parallel vectors): {cosine_similarity(vec1, vec2):.4f}\")\n",
    "print(f\"Similarity (different vectors): {cosine_similarity(vec1, vec3):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the French most visited monument?\"\n",
    "\n",
    "answers = [\n",
    "    \"The Eiffel Tower is the most visited monument in France, and is located in Paris\",\n",
    "    \"France is an amazing country, where you should live in\",\n",
    "    \"Who knows the French most visited monument?\",\n",
    "]\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, ans in enumerate(answers, 1):\n",
    "    print(f\"Answer {i}: {ans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bi-Encoder Approach\n",
    "\n",
    "Using **all-MiniLM-L6-v2**: A fast, lightweight bi-encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode query and answers independently\n",
    "query_embedding = bi_encoder.encode(query)\n",
    "answer_embeddings = bi_encoder.encode(answers)\n",
    "\n",
    "print(f\"Query embedding shape: {query_embedding.shape}\")\n",
    "print(f\"Answer embeddings shape: {answer_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity scores\n",
    "bi_encoder_scores = [\n",
    "    (cosine_similarity(query_embedding, emb), ans)\n",
    "    for emb, ans in zip(answer_embeddings, answers)\n",
    "]\n",
    "\n",
    "# Sort by score (descending)\n",
    "bi_encoder_scores.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BI-ENCODER RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "for score, answer in bi_encoder_scores:\n",
    "    print(f\"\\nScore: {score:.4f}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Encoder Approach\n",
    "\n",
    "Using **ms-marco-MiniLM-L-12-v2**: A cross-encoder trained specifically for ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "# Create (query, answer) pairs and score them\n",
    "pairs = [(query, answer) for answer in answers]\n",
    "cross_encoder_scores = cross_encoder.predict(pairs)\n",
    "\n",
    "# Combine scores with answers and sort\n",
    "cross_results = list(zip(cross_encoder_scores, answers))\n",
    "cross_results.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CROSS-ENCODER RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "for score, answer in cross_results:\n",
    "    print(f\"\\nScore: {score:.4f}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Answer\": answers,\n",
    "        \"Bi-Encoder Score\": [score for score, _ in bi_encoder_scores],\n",
    "        \"Cross-Encoder Score\": [score for score, _ in cross_results],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "| Aspect | Bi-Encoder | Cross-Encoder |\n",
    "|--------|-----------|---------------|\n",
    "| **Speed** | ✓ Fast | ✗ Slow |\n",
    "| **Scalability** | ✓ Excellent (vector DB) | ✗ Limited |\n",
    "| **Accuracy** | ✗ Lower | ✓ Higher |\n",
    "| **Pre-computation** | ✓ Yes | ✗ No |\n",
    "| **Use Case** | First-stage retrieval | Reranking top-K results |\n",
    "\n",
    "### Recommended Pipeline\n",
    "\n",
    "1. **Bi-Encoder**: Retrieve top 50-100 candidates from large corpus\n",
    "2. **Cross-Encoder**: Rerank top candidates to get best 5-10\n",
    "3. **LLM**: Generate final answer using reranked documents\n",
    "\n",
    "This hybrid approach balances speed and accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
