{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG Pipeline with AWS Bedrock\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Build a complete RAG pipeline using LangChain and AWS Bedrock\n",
    "- Understand document chunking strategies and their impact\n",
    "- Create and query a vector database with FAISS\n",
    "- Visualize embeddings in 2D space\n",
    "- Implement the retrieval-generation workflow\n",
    "\n",
    "**Inspired by:** [Hugging Face RAG Documentation](https://huggingface.co/learn/cookbook/advanced_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain langchain-aws faiss-cpu datasets langchain-community boto3 transformers umap-learn plotly umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Optional\n",
    "from langchain_aws import BedrockEmbeddings, ChatBedrock\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure AWS Bedrock\n",
    "\n",
    "**Models used:**\n",
    "- **Embeddings**: Titan Text Embeddings v2 (1024 dimensions)\n",
    "- **LLM**: Nova Lite (fast, cost-effective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_BEARER_TOKEN_BEDROCK\"] = (\n",
    "    \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "EMBEDDING_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
    "LLM_MODEL_ID = \"amazon.nova-lite-v1:0\"\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=client,\n",
    "    model_id=EMBEDDING_MODEL_ID,\n",
    ")\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    client=client,\n",
    "    model_id=LLM_MODEL_ID,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"âœ“ AWS Bedrock configured\")\n",
    "print(f\"  Embedding model: {EMBEDDING_MODEL_ID}\")\n",
    "print(f\"  LLM: {LLM_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Knowledge Base\n",
    "\n",
    "Using a subset of Hugging Face documentation as our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train[:100]\")\n",
    "\n",
    "knowledge_base = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(ds, desc=\"Loading documents\")\n",
    "]\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(knowledge_base)} documents\")\n",
    "print(f\"\\nExample document:\")\n",
    "print(f\"  Source: {knowledge_base[0].metadata['source']}\")\n",
    "print(f\"  Length: {len(knowledge_base[0].page_content)} characters\")\n",
    "print(f\"  Preview: {knowledge_base[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Chunking Strategy\n",
    "\n",
    "### Why Chunk?\n",
    "- Embedding models have maximum sequence length limits\n",
    "- Smaller chunks = more focused retrieval\n",
    "- Too small = lose context; Too large = less precise\n",
    "\n",
    "### Strategy\n",
    "We'll use **RecursiveCharacterTextSplitter** with Markdown-aware separators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=MARKDOWN_SEPARATORS,\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in tqdm(knowledge_base, desc=\"Chunking documents\"):\n",
    "    docs_processed.extend(text_splitter.split_documents([doc]))\n",
    "\n",
    "print(f\"\\nâœ“ Created {len(docs_processed)} chunks from {len(knowledge_base)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Chunk Sizes\n",
    "\n",
    "Let's check if our chunks fit within the model's token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "chunk_lengths = [\n",
    "    len(tokenizer.encode(doc.page_content))\n",
    "    for doc in tqdm(docs_processed, desc=\"Tokenizing\")\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "pd.Series(chunk_lengths).hist(bins=30, edgecolor=\"black\")\n",
    "plt.xlabel(\"Chunk Length (tokens)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Chunk Lengths\")\n",
    "plt.axvline(512, color=\"red\", linestyle=\"--\", label=\"Typical model limit (512)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nChunk length statistics:\")\n",
    "print(f\"  Mean: {pd.Series(chunk_lengths).mean():.0f} tokens\")\n",
    "print(f\"  Max: {pd.Series(chunk_lengths).max()} tokens\")\n",
    "print(f\"  Min: {pd.Series(chunk_lengths).min()} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Chunking with Token-Based Splitting\n",
    "\n",
    "If chunks are too large, we'll use a tokenizer-based splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents_optimized(\n",
    "    knowledge_base: List[Document],\n",
    "    chunk_size: int = 512,\n",
    "    tokenizer_name: str = \"bert-base-uncased\",\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents using tokenizer-based chunking.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed.extend(text_splitter.split_documents([doc]))\n",
    "\n",
    "    # Remove duplicates\n",
    "    seen = set()\n",
    "    unique_docs = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in seen:\n",
    "            seen.add(doc.page_content)\n",
    "            unique_docs.append(doc)\n",
    "\n",
    "    return unique_docs\n",
    "\n",
    "\n",
    "docs_processed = split_documents_optimized(knowledge_base, chunk_size=512)\n",
    "\n",
    "# Verify the new distribution\n",
    "chunk_lengths = [\n",
    "    len(tokenizer.encode(doc.page_content))\n",
    "    for doc in tqdm(docs_processed, desc=\"Re-tokenizing\")\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "pd.Series(chunk_lengths).hist(bins=30, edgecolor=\"black\", color=\"green\", alpha=0.7)\n",
    "plt.xlabel(\"Chunk Length (tokens)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Optimized Chunk Length Distribution\")\n",
    "plt.axvline(512, color=\"red\", linestyle=\"--\", label=\"Target limit (512)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Optimized to {len(docs_processed)} chunks\")\n",
    "print(f\"  Max length: {max(chunk_lengths)} tokens (within limit!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Vector Database with FAISS\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a fast library for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "print(\"Creating FAISS vector database...\")\n",
    "print(\"(This may take a few minutes to embed all documents)\\n\")\n",
    "\n",
    "vector_db = FAISS.from_documents(\n",
    "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Vector database created\")\n",
    "print(f\"  Total vectors: {vector_db.index.ntotal}\")\n",
    "print(f\"  Dimension: {vector_db.index.d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Embeddings in 2D\n",
    "\n",
    "Let's visualize how documents and queries are positioned in embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How to create a pipeline object?\n",
      "Query embedding dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "user_query = \"How to create a pipeline object?\"\n",
    "query_vector = embedding_model.embed_query(user_query)\n",
    "\n",
    "print(f\"Query: {user_query}\")\n",
    "print(f\"Query embedding dimension: {len(query_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Collect all embeddings (documents + query)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m n_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(docs_processed)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from umap import UMAP\n",
    "\n",
    "# Collect all embeddings (documents + query)\n",
    "n_docs = len(docs_processed)\n",
    "all_embeddings = []\n",
    "\n",
    "for idx in range(n_docs):\n",
    "    doc_vector = vector_db.index.reconstruct(idx)\n",
    "    all_embeddings.append(doc_vector)\n",
    "\n",
    "all_embeddings.append(query_vector)\n",
    "embeddings_array = np.array(all_embeddings)\n",
    "\n",
    "# Reduce to 2D using UMAP\n",
    "print(\"Reducing dimensions with UMAP...\")\n",
    "n_neighbors = min(15, max(2, n_docs - 1))\n",
    "reducer = UMAP(n_neighbors=n_neighbors, n_components=2, random_state=42, min_dist=0.1)\n",
    "embeddings_2d = reducer.fit_transform(embeddings_array)\n",
    "\n",
    "print(f\"âœ“ Reduced from {embeddings_array.shape[1]}D to 2D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Separate documents and query\n",
    "doc_x, doc_y = embeddings_2d[:-1, 0], embeddings_2d[:-1, 1]\n",
    "query_x, query_y = embeddings_2d[-1, 0], embeddings_2d[-1, 1]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Documents\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=doc_x,\n",
    "        y=doc_y,\n",
    "        mode=\"markers\",\n",
    "        name=\"Documents\",\n",
    "        marker=dict(size=6, color=\"lightblue\", opacity=0.6),\n",
    "        text=[f\"Doc {i}\" for i in range(len(doc_x))],\n",
    "        hovertemplate=\"<b>%{text}</b><extra></extra>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Query\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[query_x],\n",
    "        y=[query_y],\n",
    "        mode=\"markers\",\n",
    "        name=\"Query\",\n",
    "        marker=dict(size=15, color=\"red\", symbol=\"star\"),\n",
    "        text=[user_query],\n",
    "        hovertemplate=\"<b>Query:</b> %{text}<extra></extra>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"2D Visualization of Document Embeddings\",\n",
    "    xaxis_title=\"Dimension 1\",\n",
    "    yaxis_title=\"Dimension 2\",\n",
    "    width=900,\n",
    "    height=600,\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Red star = query | Blue dots = documents\")\n",
    "print(f\"Closer documents are semantically more similar to the query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieval: Query the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query: {user_query}\\n\")\n",
    "\n",
    "retrieved_docs = vector_db.similarity_search(query=user_query, k=5)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 5 RETRIEVED DOCUMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n[Rank {i}]\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content: {doc.page_content[:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generation: LLM Response with Retrieved Context\n",
    "\n",
    "Now we'll use the retrieved documents as context for the LLM to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a helpful assistant. Use the provided context to answer the user's question.\n",
    "        \n",
    "Rules:\n",
    "- Answer only based on the context provided\n",
    "- Be concise and relevant\n",
    "- Cite document numbers when referencing information\n",
    "- If the answer is not in the context, say so\"\"\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"âœ“ RAG prompt template created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare context from retrieved documents\n",
    "context = \"\\n\\n\".join(\n",
    "    [f\"Document {i}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs, 1)]\n",
    ")\n",
    "\n",
    "# Format prompt\n",
    "messages = RAG_PROMPT.format_messages(question=user_query, context=context)\n",
    "\n",
    "# Generate answer\n",
    "print(\"Generating answer...\\n\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RAG ANSWER\")\n",
    "print(\"=\" * 80)\n",
    "print(response.content)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete RAG Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query: str, k: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: retrieve relevant docs and generate answer.\n",
    "\n",
    "    Args:\n",
    "        query: User's question\n",
    "        k: Number of documents to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Dict with answer, retrieved docs, and metadata\n",
    "    \"\"\"\n",
    "    # Retrieve\n",
    "    retrieved_docs = vector_db.similarity_search(query=query, k=k)\n",
    "\n",
    "    # Prepare context\n",
    "    context = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Document {i}:\\n{doc.page_content}\"\n",
    "            for i, doc in enumerate(retrieved_docs, 1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    messages = RAG_PROMPT.format_messages(question=query, context=context)\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": response.content,\n",
    "        \"retrieved_docs\": retrieved_docs,\n",
    "        \"num_docs\": len(retrieved_docs),\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the pipeline\n",
    "test_queries = [\n",
    "    \"How to create a pipeline object?\",\n",
    "    \"What is the purpose of tokenizers?\",\n",
    "    \"How to fine-tune a model?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    result = rag_query(query, k=3)\n",
    "    print(f\"Answer: {result['answer'][:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### RAG Pipeline Components\n",
    "\n",
    "1. **Document Processing**\n",
    "   - Load knowledge base\n",
    "   - Chunk documents (token-aware)\n",
    "   - Remove duplicates\n",
    "\n",
    "2. **Indexing**\n",
    "   - Embed documents\n",
    "   - Store in vector database (FAISS)\n",
    "   - Enable fast similarity search\n",
    "\n",
    "3. **Retrieval**\n",
    "   - Embed query\n",
    "   - Find k most similar documents\n",
    "   - Return relevant context\n",
    "\n",
    "4. **Generation**\n",
    "   - Format retrieved docs as context\n",
    "   - Create prompt for LLM\n",
    "   - Generate grounded answer\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Use tokenizer-based chunking for consistent sizes\n",
    "- Choose chunk size based on model limits and task\n",
    "- Optimize k based on context window and relevance\n",
    "- Monitor chunk distribution to avoid truncation\n",
    "- Use appropriate distance metrics (cosine for normalized embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
