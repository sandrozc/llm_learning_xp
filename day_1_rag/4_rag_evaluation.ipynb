{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation: Measuring Retrieval and Generation Quality\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand key metrics for evaluating RAG systems\n",
    "- Measure retrieval quality (precision, recall)\n",
    "- Evaluate generation quality (faithfulness, relevance)\n",
    "- Implement practical evaluation pipelines\n",
    "- Learn best practices for RAG evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers numpy pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sample Dataset\n",
    "\n",
    "Let's create a small knowledge base with questions and ground truth answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base documents\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": 0,\n",
    "        \"text\": \"Python is a high-level programming language known for its simplicity and readability.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"text\": \"Machine learning is a subset of AI that enables computers to learn from data.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"text\": \"Neural networks are computing systems inspired by biological neural networks.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"text\": \"Deep learning uses multi-layered neural networks to process complex patterns.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"text\": \"Natural language processing enables computers to understand human language.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"text\": \"Transformers are a type of neural network architecture for NLP tasks.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"text\": \"RAG combines retrieval and generation for knowledge-intensive tasks.\",\n",
    "    },\n",
    "    {\"id\": 7, \"text\": \"Vector databases store embeddings for fast similarity search.\"},\n",
    "    {\"id\": 8, \"text\": \"BERT is a transformer model pre-trained on large text corpora.\"},\n",
    "    {\"id\": 9, \"text\": \"Fine-tuning adapts pre-trained models to specific tasks.\"},\n",
    "]\n",
    "\n",
    "# Test queries with ground truth relevant document IDs\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"What is machine learning?\",\n",
    "        \"relevant_docs\": [1, 3],  # Ground truth\n",
    "        \"expected_answer\": \"Machine learning is a subset of AI that enables computers to learn from data.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain neural networks\",\n",
    "        \"relevant_docs\": [2, 3],\n",
    "        \"expected_answer\": \"Neural networks are computing systems inspired by biological neural networks that can learn patterns.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are transformers in NLP?\",\n",
    "        \"relevant_docs\": [5, 8],\n",
    "        \"expected_answer\": \"Transformers are a type of neural network architecture designed for NLP tasks.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How does RAG work?\",\n",
    "        \"relevant_docs\": [6, 7],\n",
    "        \"expected_answer\": \"RAG combines retrieval from knowledge bases and generation to answer knowledge-intensive questions.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base: {len(documents)} documents\")\n",
    "print(f\"Test queries: {len(test_queries)} queries\")\n",
    "print(\"\\nExample query:\")\n",
    "print(f\"  Q: {test_queries[0]['query']}\")\n",
    "print(f\"  Relevant docs: {test_queries[0]['relevant_docs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Retrieval System\n",
    "\n",
    "Create a basic retrieval system using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRetriever:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.documents = None\n",
    "        self.doc_embeddings = None\n",
    "\n",
    "    def index(self, documents: List[Dict]):\n",
    "        \"\"\"Index documents by computing their embeddings.\"\"\"\n",
    "        self.documents = documents\n",
    "        texts = [doc[\"text\"] for doc in documents]\n",
    "        self.doc_embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "        print(f\"✓ Indexed {len(documents)} documents\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Retrieve top-k most similar documents.\"\"\"\n",
    "        query_embedding = self.model.encode(query, convert_to_numpy=True)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarities = np.dot(self.doc_embeddings, query_embedding) / (\n",
    "            np.linalg.norm(self.doc_embeddings, axis=1)\n",
    "            * np.linalg.norm(query_embedding)\n",
    "        )\n",
    "\n",
    "        # Get top-k indices\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "\n",
    "        # Return documents with scores\n",
    "        results = [\n",
    "            {\n",
    "                \"doc_id\": self.documents[idx][\"id\"],\n",
    "                \"text\": self.documents[idx][\"text\"],\n",
    "                \"score\": float(similarities[idx]),\n",
    "                \"rank\": rank + 1,\n",
    "            }\n",
    "            for rank, idx in enumerate(top_k_indices)\n",
    "        ]\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "retriever = SimpleRetriever()\n",
    "retriever.index(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieval Evaluation Metrics\n",
    "\n",
    "### Key Metrics:\n",
    "- **Precision@K**: What fraction of retrieved docs are relevant?\n",
    "- **Recall@K**: What fraction of relevant docs were retrieved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(\n",
    "    retrieved_ids: List[int], relevant_ids: List[int], k: int = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate Precision@K.\n",
    "\n",
    "    Formula: |retrieved ∩ relevant| / k\n",
    "    \"\"\"\n",
    "    if k:\n",
    "        retrieved_ids = retrieved_ids[:k]\n",
    "\n",
    "    relevant_retrieved = len(set(retrieved_ids) & set(relevant_ids))\n",
    "    return relevant_retrieved / len(retrieved_ids) if retrieved_ids else 0.0\n",
    "\n",
    "\n",
    "def recall_at_k(\n",
    "    retrieved_ids: List[int], relevant_ids: List[int], k: int = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate Recall@K.\n",
    "\n",
    "    Formula: |retrieved ∩ relevant| / |relevant|\n",
    "    \"\"\"\n",
    "    if k:\n",
    "        retrieved_ids = retrieved_ids[:k]\n",
    "\n",
    "    relevant_retrieved = len(set(retrieved_ids) & set(relevant_ids))\n",
    "    return relevant_retrieved / len(relevant_ids) if relevant_ids else 0.0\n",
    "\n",
    "\n",
    "# Test the metrics with an example\n",
    "example_retrieved = [1, 5, 2, 7]\n",
    "example_relevant = [1, 3, 2]\n",
    "\n",
    "print(\"Example Evaluation:\")\n",
    "print(f\"  Retrieved: {example_retrieved}\")\n",
    "print(f\"  Relevant:  {example_relevant}\")\n",
    "print(\n",
    "    f\"\\n  Precision@4: {precision_at_k(example_retrieved, example_relevant, k=4):.3f}\"\n",
    ")\n",
    "print(f\"  Recall@4:    {recall_at_k(example_retrieved, example_relevant, k=4):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(retriever, test_queries: List[Dict], k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate retriever on all test queries.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for test_case in test_queries:\n",
    "        query = test_case[\"query\"]\n",
    "        relevant_ids = test_case[\"relevant_docs\"]\n",
    "\n",
    "        # Retrieve documents\n",
    "        retrieved = retriever.retrieve(query, k=k)\n",
    "        retrieved_ids = [doc[\"doc_id\"] for doc in retrieved]\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"query\": query,\n",
    "            \"retrieved_ids\": retrieved_ids,\n",
    "            \"relevant_ids\": relevant_ids,\n",
    "            \"precision@3\": precision_at_k(retrieved_ids, relevant_ids, k=3),\n",
    "            \"recall@3\": recall_at_k(retrieved_ids, relevant_ids, k=3),\n",
    "            \"precision@5\": precision_at_k(retrieved_ids, relevant_ids, k=5),\n",
    "            \"recall@5\": recall_at_k(retrieved_ids, relevant_ids, k=5),\n",
    "        }\n",
    "        results.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "eval_results = evaluate_retrieval(retriever, test_queries, k=5)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"RETRIEVAL EVALUATION RESULTS\")\n",
    "print(\"=\" * 100)\n",
    "print(eval_results[[\"query\", \"precision@3\", \"recall@3\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"AVERAGE METRICS\")\n",
    "print(\"=\" * 100)\n",
    "avg_metrics = eval_results[\n",
    "    [\"precision@3\", \"recall@3\", \"precision@5\", \"recall@5\"]\n",
    "].mean()\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Retrieval Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics_to_plot = [\"precision@3\", \"recall@3\", \"precision@5\", \"recall@5\"]\n",
    "avg_values = [eval_results[m].mean() for m in metrics_to_plot]\n",
    "colors = [\"steelblue\", \"lightblue\", \"darkgreen\", \"lightgreen\"]\n",
    "\n",
    "ax.bar(range(len(metrics_to_plot)), avg_values, color=colors, alpha=0.7)\n",
    "ax.set_xticks(range(len(metrics_to_plot)))\n",
    "ax.set_xticklabels([\"P@3\", \"R@3\", \"P@5\", \"R@5\"])\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Precision and Recall at K=3 and K=5\", fontweight=\"bold\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete RAG Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_system(\n",
    "    retriever, test_queries: List[Dict], k: int = 5\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Complete evaluation pipeline for RAG system.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with retrieval and generation evaluation dataframes\n",
    "    \"\"\"\n",
    "    gen_evaluator = GenerationEvaluator()\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for test_case in test_queries:\n",
    "        query = test_case[\"query\"]\n",
    "        relevant_ids = test_case[\"relevant_docs\"]\n",
    "        expected_answer = test_case[\"expected_answer\"]\n",
    "\n",
    "        # Retrieval\n",
    "        retrieved_docs = retriever.retrieve(query, k=k)\n",
    "        retrieved_ids = [doc[\"doc_id\"] for doc in retrieved_docs]\n",
    "        context = \" \".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "\n",
    "        # Retrieval metrics\n",
    "        retrieval_metrics = {\n",
    "            \"precision@3\": precision_at_k(retrieved_ids, relevant_ids, k=3),\n",
    "            \"recall@3\": recall_at_k(retrieved_ids, relevant_ids, k=3),\n",
    "        }\n",
    "\n",
    "        # Generation\n",
    "        generated_answer = simulate_rag_answer(query, retrieved_docs)\n",
    "\n",
    "        # Combine\n",
    "        result = {\"query\": query, **retrieval_metrics}\n",
    "        all_results.append(result)\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Run complete evaluation\n",
    "complete_eval = evaluate_rag_system(retriever, test_queries, k=5)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"COMPLETE RAG SYSTEM EVALUATION\")\n",
    "print(\"=\" * 100)\n",
    "print(complete_eval.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"OVERALL SYSTEM PERFORMANCE\")\n",
    "print(\"=\" * 100)\n",
    "numeric_cols = complete_eval.select_dtypes(include=[np.number]).columns\n",
    "avg_all = complete_eval[numeric_cols].mean()\n",
    "for metric, value in avg_all.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "gs = fig.add_gridspec(2, 1, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Retrieval metrics bar chart (changed from polar)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "retrieval_metrics = [\"precision@3\", \"recall@3\"]\n",
    "values = [complete_eval[m].mean() for m in retrieval_metrics]\n",
    "bars = ax1.bar(\n",
    "    range(len(retrieval_metrics)), values, color=[\"steelblue\", \"lightblue\"], alpha=0.7\n",
    ")\n",
    "ax1.set_xticks(range(len(retrieval_metrics)))\n",
    "ax1.set_xticklabels([\"P@3\", \"R@3\"])\n",
    "ax1.set_ylabel(\"Score\")\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_title(\"Retrieval Performance\", fontweight=\"bold\")\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height,\n",
    "        f\"{height:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "# 2. Overall score gauge\n",
    "ax2 = fig.add_subplot(gs[1, :])\n",
    "overall_score = complete_eval[numeric_cols].mean().mean()\n",
    "ax2.text(\n",
    "    0.5,\n",
    "    0.6,\n",
    "    f\"{overall_score:.2%}\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=60,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"darkgreen\",\n",
    ")\n",
    "ax2.text(0.5, 0.3, \"Overall RAG System Score\", ha=\"center\", va=\"center\", fontsize=20)\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "# Add performance category\n",
    "if overall_score >= 0.8:\n",
    "    category = \"Excellent\"\n",
    "    color = \"green\"\n",
    "elif overall_score >= 0.6:\n",
    "    category = \"Good\"\n",
    "    color = \"orange\"\n",
    "else:\n",
    "    category = \"Needs Improvement\"\n",
    "    color = \"red\"\n",
    "\n",
    "ax2.text(\n",
    "    0.5,\n",
    "    0.15,\n",
    "    category,\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=16,\n",
    "    color=color,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "plt.suptitle(\"RAG System Evaluation Dashboard\", fontsize=18, fontweight=\"bold\", y=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Create ground truth datasets** with labeled relevant documents\n",
    "2. **Evaluate both retrieval and generation** separately\n",
    "3. **Track metrics over time** to monitor improvements\n",
    "4. **Use multiple metrics** for comprehensive evaluation\n",
    "5. **Consider task-specific metrics** based on your use case\n",
    "6. **A/B test changes** before deploying to production\n",
    "7. **Collect user feedback** as the ultimate evaluation\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "- **Low Precision**: Retriever returns irrelevant docs → Improve embeddings or add reranking\n",
    "- **Low Recall**: Missing relevant docs → Increase K or improve chunking strategy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
